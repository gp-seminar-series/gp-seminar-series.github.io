+++
title = "On Riemannian Latent Variable Models and Pullback Metrics"
[extra]
author = "Noémie Jaquier"
institution = "KTH Royal Institute of Technology"
author_url = "https://njaquier.ch"
highlight = false
+++

Latent variable models are powerful tools for learning low-dimensional representations of complex, high-dimensional data. While these models typically consider Euclidean data and latent spaces, geometrically-constrained, a.k.a. Riemannian, data appears in many application domains, including robotics, biology, and medical imaging. Moreover, non-Euclidean latent spaces are essential to preserve the relationships between the original datapoints. In this talk, I will first present a generative model that learns low-dimensional Riemannian latent representation from Riemannian data. I will explain how the latent space can be equipped with the pullback metric induced by a wrapped Gaussian process latent variable model, which accounts for the intrinsic geometry of the data. In the second part of the talk, I will discuss how additional priors can naturally be included by imposing a given Riemannian geometry to the latent space. Specifically, I will present a Gaussian process hyperbolic latent variable model with hyperbolic latent space that learns continuous embeddings of data with a hierarchical structure. Finally, I will show that the hyperbolic metric of the latent space can be augmented with a pullback metric to account for the data manifold. The resulting geodesics not only respect the geometry of the hyperbolic latent space but also align with the underlying data distribution, significantly reducing uncertainty in predictions.

Noémie Jaquier is an assistant professor at the KTH Royal Institute of Technology and the head of the Geometric Robot (GeoRob) Lab at the Division of Robotics, Perception and Learning. She received her PhD degree from the Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland in 2020. Prior to joining KTH, she was a postdoctoral researcher in the High Performance Humanoid Technologies Lab (H²T) at the Karlsruhe Institute of Technology (KIT) and a visiting postdoctoral scholar at the Stanford Robotics Lab. Her research investigates data-efficient and theoretically-sound learning algorithms that leverage differential geometry- and physics-based inductive bias to endow robots with close-to-human learning and adaptation capabilities.
